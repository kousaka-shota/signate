{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/bert_nlp/blob/main/section_5/01_news_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgegdDZjf8E"
      },
      "source": [
        "# 日本語文章の分類\n",
        "\n",
        "日本語のデータセットでBERTのモデルをファインチューニングし、ニュースの分類を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 大きな流れ\n",
        "①ディレクトリにあるテキストファイルをidと本文をセットにしてリスト化  <br>\n",
        "②上記を訓練データとテストデータに分割してCSV化  <br>\n",
        "③日本語の学習モデル（分類する）と日本語のトークナイザー（形態素分析）の読み込み <br>\n",
        "④データセットの読み込み  <br>\n",
        "⑤評価関数の定義  <br>\n",
        "⑥Trainerの設定（ハイパーパラメータの設定） <br>\n",
        "⑦モデルの訓練  <br>\n",
        "⑧モデルの評価  <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6moZnLFkFwr"
      },
      "source": [
        "## ライブラリのインストール\n",
        "ライブラリTransformers、およびnlpをインストールします。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7qg6t5nnBjqs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
            "  Using cached huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from transformers) (23.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached PyYAML-6.0.1-cp38-cp38-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Using cached regex-2023.12.25-cp38-cp38-win_amd64.whl.metadata (41 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Using cached tokenizers-0.15.0-cp38-none-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Using cached safetensors-0.4.1-cp38-none-win_amd64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
            "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
            "  Using cached charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "Using cached huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
            "Using cached PyYAML-6.0.1-cp38-cp38-win_amd64.whl (157 kB)\n",
            "Using cached regex-2023.12.25-cp38-cp38-win_amd64.whl (269 kB)\n",
            "Using cached safetensors-0.4.1-cp38-none-win_amd64.whl (277 kB)\n",
            "Using cached tokenizers-0.15.0-cp38-none-win_amd64.whl (2.2 MB)\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
            "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
            "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.2 idna-3.6 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.36.2 urllib3-2.1.0\n",
            "Collecting nlp\n",
            "  Using cached nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: numpy in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from nlp) (1.24.4)\n",
            "Collecting pyarrow>=0.16.0 (from nlp)\n",
            "  Using cached pyarrow-14.0.2-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
            "Collecting dill (from nlp)\n",
            "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from nlp) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from nlp) (4.66.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from nlp) (3.13.1)\n",
            "Collecting xxhash (from nlp)\n",
            "  Using cached xxhash-3.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->nlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->nlp) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->nlp) (2023.11.17)\n",
            "Requirement already satisfied: colorama in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from tqdm>=4.27->nlp) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from pandas->nlp) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from pandas->nlp) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.16.0)\n",
            "Using cached pyarrow-14.0.2-cp38-cp38-win_amd64.whl (24.6 MB)\n",
            "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Using cached xxhash-3.4.1-cp38-cp38-win_amd64.whl (29 kB)\n",
            "Installing collected packages: xxhash, pyarrow, dill, nlp\n",
            "Successfully installed dill-0.3.7 nlp-0.4.0 pyarrow-14.0.2 xxhash-3.4.1\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (14.0.2)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Using cached multiprocess-0.70.15-py38-none-any.whl.metadata (7.1 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Using cached aiohttp-3.9.1-cp38-cp38-win_amd64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Using cached multidict-6.0.4-cp38-cp38-win_amd64.whl (28 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Using cached yarl-1.9.4-cp38-cp38-win_amd64.whl.metadata (32 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Using cached frozenlist-1.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: colorama in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "Using cached aiohttp-3.9.1-cp38-cp38-win_amd64.whl (367 kB)\n",
            "Using cached multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
            "Using cached yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
            "Installing collected packages: pyarrow-hotfix, multiprocess, multidict, fsspec, frozenlist, attrs, async-timeout, yarl, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.12.2\n",
            "    Uninstalling fsspec-2023.12.2:\n",
            "      Successfully uninstalled fsspec-2023.12.2\n",
            "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 datasets-2.16.1 frozenlist-1.4.1 fsspec-2023.10.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-hotfix-0.6 yarl-1.9.4\n",
            "Collecting fugashi\n",
            "  Using cached fugashi-1.3.0-cp38-cp38-win_amd64.whl.metadata (7.1 kB)\n",
            "Using cached fugashi-1.3.0-cp38-cp38-win_amd64.whl (500 kB)\n",
            "Installing collected packages: fugashi\n",
            "Successfully installed fugashi-1.3.0\n",
            "Collecting ipadic\n",
            "  Using cached ipadic-1.0.0-py3-none-any.whl\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.99-cp38-cp38-win_amd64.whl (977 kB)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from accelerate) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from accelerate) (5.9.7)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from accelerate) (6.0.1)\n",
            "Collecting torch>=1.10.0 (from accelerate)\n",
            "  Using cached torch-2.1.2-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from accelerate) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Collecting sympy (from torch>=1.10.0->accelerate)\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting networkx (from torch>=1.10.0->accelerate)\n",
            "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "Collecting jinja2 (from torch>=1.10.0->accelerate)\n",
            "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: fsspec in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->accelerate)\n",
            "  Using cached MarkupSafe-2.1.3-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1573207\\git\\signate\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->accelerate)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "Using cached torch-2.1.2-cp38-cp38-win_amd64.whl (192.3 MB)\n",
            "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "Using cached MarkupSafe-2.1.3-cp38-cp38-win_amd64.whl (17 kB)\n",
            "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch, accelerate\n",
            "Successfully installed MarkupSafe-2.1.3 accelerate-0.26.1 jinja2-3.1.3 mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nlp\n",
        "!pip install datasets\n",
        "!pip install fugashi\n",
        "!pip install ipadic\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHOX9LyZc2g"
      },
      "source": [
        "## Google ドライブとの連携  \n",
        "以下のコードを実行し、認証コードを使用してGoogle ドライブをマウントします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h7BA67Ed5wT"
      },
      "outputs": [],
      "source": [
        "# ローカルにおいている為不要\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zliYGLC5g0h2"
      },
      "source": [
        "## データセットの読み込み\n",
        "Googleドライブに保存されている、ニュースのデータセットを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jPV3qCYs9STS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "files: 4451dirs: 9"
          ]
        }
      ],
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "\n",
        "path = \"./text/\"  # フォルダの場所を指定\n",
        "\n",
        "dir_files = os.listdir(path=path)\n",
        "dirs = [f for f in dir_files if os.path.isdir(os.path.join(path, f))]  # ディレクトリ一覧\n",
        "\n",
        "text_label_data = []  # 文章とラベルのセット\n",
        "dir_count = 0  # ディレクトリ数のカウント\n",
        "file_count= 0  # ファイル数のカウント\n",
        "\n",
        "# ディレクトリ内のtxtファイルを取得\n",
        "for i in range(len(dirs)):\n",
        "    dir = dirs[i]\n",
        "    files = glob.glob(path + dir + \"/*.txt\")  # ファイルの一覧\n",
        "    dir_count += 1\n",
        "\n",
        "    for file in files:\n",
        "        # licence.txtは不要なので無視\n",
        "        if os.path.basename(file) == \"LICENSE.txt\":\n",
        "            continue\n",
        "\n",
        "        with open(file, \"r\", encoding='utf-8') as f:\n",
        "            # 指定の行だけ読み込み（先頭の2行が不要なので除去）\n",
        "            text = f.readlines()[3:]\n",
        "            # 結合\n",
        "            text = \"\".join(text)\n",
        "            # 改行やタブを除去\n",
        "            text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"}))\n",
        "            # 本文とラベルをリストに格納\n",
        "            text_label_data.append([text, i])\n",
        "\n",
        "        file_count += 1\n",
        "        print(\"\\rfiles: \" + str(file_count) + \"dirs: \" + str(dir_count), end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADy70wOgyXg"
      },
      "source": [
        "## データの保存\n",
        "データを訓練データとテストデータに分割し、csvファイルとしてGoogle Driveに保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fIyvN2MT4Unl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "news_train, news_test =  train_test_split(text_label_data, shuffle=True)  # 訓練用とテスト用に分割\n",
        "news_path = \"./train_test_text/\"\n",
        "\n",
        "with open(news_path+\"news_train.csv\", \"w\",encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(news_train)\n",
        "\n",
        "with open(news_path+\"news_test.csv\", \"w\",encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(news_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsgQNMJxpBnW"
      },
      "source": [
        "## モデルとTokenizerの読み込み\n",
        "日本語の事前学習済みモデルと、これと紐づいたTokenizerを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9R0HK29fHrf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100%|██████████| 110/110 [00:00<00:00, 3.23kB/s]\n",
            "vocab.txt: 100%|██████████| 258k/258k [00:00<00:00, 1.51MB/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
        "# num_label=9は9個に分類するよーってこと\n",
        "sc_model = BertForSequenceClassification.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\", num_labels=9)\n",
        "# sc_model.cuda()\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCmm2TjqToE"
      },
      "source": [
        "## データセットの読み込み\n",
        "保存されたニュースのデータを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rfEnNpv9HuXI"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Unable to find '/news_train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m news_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# splitは訓練データとテストデータを分割する場合は指定\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# データの読み込み\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnews_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnews_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 各行を単語ごとに分割（各行にtokenize関数を実行）\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mmap(tokenize, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data))\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\load.py:2523\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2518\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2519\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2520\u001b[0m )\n\u001b[0;32m   2522\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2523\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2540\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\load.py:2195\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   2194\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 2195\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\load.py:1730\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \n\u001b[0;32m   1728\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m-> 1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\load.py:1120\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1118\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[0;32m   1119\u001b[0m patterns \u001b[38;5;241m=\u001b[39m sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[1;32m-> 1120\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\data_files.py:689\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    686\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    688\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 689\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[0;32m    697\u001b[0m     )\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\data_files.py:594\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    593\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 594\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m         )\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
            "File \u001b[1;32mc:\\Users\\1573207\\Git\\signate\\venv\\lib\\site-packages\\datasets\\data_files.py:383\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[1;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to find '/news_train.csv'"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def tokenize(batch):\n",
        "    # max_lenghtは処理を早くするために128にしているが文章の長さに応じて指定\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "news_path = \"/train_test_text/\"\n",
        "# splitは訓練データとテストデータを分割する場合は指定\n",
        "# データの読み込み\n",
        "train_data = load_dataset(\"csv\", data_files=news_path+\"news_train.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "# 各行を単語ごとに分割（各行にtokenize関数を実行）\n",
        "train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
        "# pytorchなのでtorch\n",
        "train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "test_data = load_dataset(\"csv\", data_files=news_path+\"news_test.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n",
        "test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y6Fcqmy2rG2"
      },
      "source": [
        "## 評価用の関数\n",
        "`sklearn.metrics`を使用し、モデルを評価するための関数を定義します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plAZjdkG0FdV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(result):\n",
        "    labels = result.label_ids\n",
        "    preds = result.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLqAVy7z0T3"
      },
      "source": [
        "## Trainerの設定\n",
        "Trainerクラス、およびTrainingArgumentsクラスを使用して、訓練を行うTrainerの設定を行います。\n",
        "https://huggingface.co/transformers/main_classes/trainer.html   \n",
        "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhaexaAOI3kV"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 2,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n",
        "    weight_decay = 0.01,  # 重みの減衰率\n",
        "    logging_dir = \"./logs\",\n",
        "    evaluation_strategy = \"steps\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = sc_model,\n",
        "    args = training_args,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = test_data,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0F5nXKpSCnS"
      },
      "source": [
        "## モデルの訓練\n",
        "設定に基づきファインチューニングを行います。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29fkN4UcI4jm"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c76zhkQVS2xZ"
      },
      "source": [
        "## モデルの評価\n",
        "Trainerの`evaluate()`メソッドによりモデルを評価します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIgke21zI6l_"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EFwqzLRUhaB"
      },
      "source": [
        "## TensorBoardによる結果の表示\n",
        "TensorBoardを使って、logsフォルダに格納された学習過程を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vv39tuDJq5n"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-BscHjHxs0H"
      },
      "source": [
        "## モデルの保存\n",
        "訓練済みのモデルを保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvwVcXuIyH7V"
      },
      "outputs": [],
      "source": [
        "news_path = \"/content/drive/My Drive/bert_nlp/section_5/\"\n",
        "\n",
        "sc_model.save_pretrained(news_path)\n",
        "tokenizer.save_pretrained(news_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZuJCZBK0RJx"
      },
      "source": [
        "## モデルの読み込み\n",
        "保存済みのモデルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWtcQRuP0X45"
      },
      "outputs": [],
      "source": [
        "loaded_model = BertForSequenceClassification.from_pretrained(news_path)\n",
        "loaded_model.cuda()\n",
        "loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(news_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq2zZ99R3Hs7"
      },
      "source": [
        "## 日本語ニュースの分類\n",
        "読み込んだモデルを使ってニュースを分類します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFOIjY511WVK"
      },
      "outputs": [],
      "source": [
        "import glob  # ファイルの取得に使用\n",
        "import os\n",
        "import torch\n",
        "\n",
        "category = \"movie-enter\"\n",
        "sample_path = \"/content/drive/My Drive/bert_nlp/section_5/text/\"  # フォルダの場所を指定\n",
        "files = glob.glob(sample_path + category + \"/*.txt\")  # ファイルの一覧\n",
        "file = files[12]  # 適当なニュース\n",
        "\n",
        "dir_files = os.listdir(path=sample_path)\n",
        "dirs = [f for f in dir_files if os.path.isdir(os.path.join(sample_path, f))]  # ディレクトリ一覧\n",
        "\n",
        "with open(file, \"r\") as f:\n",
        "    sample_text = f.readlines()[3:]\n",
        "    sample_text = \"\".join(sample_text)\n",
        "    sample_text = sample_text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"}))\n",
        "\n",
        "print(sample_text)\n",
        "\n",
        "max_length = 512\n",
        "words = loaded_tokenizer.tokenize(sample_text)\n",
        "word_ids = loaded_tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "word_tensor = torch.tensor([word_ids[:max_length]])  # テンソルに変換\n",
        "\n",
        "x = word_tensor.cuda()  # GPU対応\n",
        "y = loaded_model(x)  # 予測\n",
        "pred = y[0].argmax(-1)  # 最大値のインデックス\n",
        "print(\"result:\", dirs[pred])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPlrd3J9KHmkTu+Pmer62Hr",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
